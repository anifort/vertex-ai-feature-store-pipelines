{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6657d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import time, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6625ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    Artifact,\n",
    "    Model,\n",
    "    ClassificationMetrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e2fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-bigquery==2.24.1\"])\n",
    "def data_selector(\n",
    "    query: str,\n",
    "    bq_project_id: str,\n",
    "    bq_dataset_id: str,\n",
    "    bq_table_id: str,\n",
    "    bq_location: str,\n",
    "    table_dataset: Output[Dataset]\n",
    ") -> None:\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    bq_query_data_table=\"{project}.{dataset}.{table}\".format(\n",
    "        project=bq_project_id, \n",
    "        dataset=bq_dataset_id, \n",
    "        table=bq_table_id)\n",
    "\n",
    "    client = bigquery.Client(project=bq_project_id, location=bq_location,)\n",
    "\n",
    "    overwrite_table = False\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE if overwrite_table else bigquery.job.WriteDisposition.WRITE_EMPTY,\n",
    "        destination = bq_query_data_table)\n",
    "\n",
    "    try:\n",
    "        query_job = client.query(query = query, \n",
    "                                 job_config = job_config)\n",
    "        query_job.result()\n",
    "        #if .total_rows == 0:\n",
    "        #    raise Exception(\"Query return no rows\".format(bq_query_data_table))\n",
    "\n",
    "        if query_job.errors: \n",
    "            raise Exception() \n",
    "    except Exception as e:\n",
    "        logging.error(query_job.errors)\n",
    "        raise e\n",
    "\n",
    "    \n",
    "\n",
    "    table = client.get_table(bq_query_data_table)  # Make an API request.\n",
    "    table_dataset.path = \"bq://{}\".format(bq_query_data_table)\n",
    "    table_dataset.metadata['table_name'] = bq_query_data_table\n",
    "    \n",
    "    print(vars(table_dataset))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# View table properties\n",
    "#print(\"Table schema: {}\".format(table.schema))\n",
    "#print(\"Table description: {}\".format(table.description))\n",
    "#print(\"Table has {} rows\".format(table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b2a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT planet as planets, terrestrial_date as timestamp, 5 as pt\n",
    "        FROM `feature-store-mars21.mars.three_planets_tmp` WHERE 1=1\n",
    "    \"\"\"\n",
    "\n",
    "bq_location = 'US'\n",
    "bq_project_id = \"feature-store-mars21\"\n",
    "bq_dataset_id = \"mars\"\n",
    "bq_table_id = \"tmp-table-v14\"\n",
    "\n",
    "bq_export_table_id = \"training-v1\"\n",
    "    \n",
    "\n",
    "BUCKET_NAME = \"gs://feature-store-mars21\"\n",
    "@dsl.pipeline(\n",
    "  name='bq-fs-export',\n",
    "  description='',\n",
    "  pipeline_root=BUCKET_NAME+\"/xgb-pl\"\n",
    ")\n",
    "def pipeline(\n",
    "    query: str,\n",
    "    bq_project_id: str,\n",
    "    bq_dataset_id: str,\n",
    "    bq_table_id: str,\n",
    "    bq_location: str\n",
    "):\n",
    "    \n",
    "    prepro_op = data_selector(\n",
    "        query,\n",
    "        bq_project_id,\n",
    "        bq_dataset_id,\n",
    "        bq_table_id,\n",
    "        bq_location)\n",
    "    \n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"pl.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea24787",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "\n",
    "params['query'] = \"\"\"\n",
    "    SELECT planet as planets, terrestrial_date as timestamp, 5 as pt\n",
    "        FROM `feature-store-mars21.mars.three_planets_tmp` WHERE 1=1\n",
    "    \"\"\"\n",
    "\n",
    "params['bq_location'] = 'US'\n",
    "params['bq_project_id'] = \"feature-store-mars21\"\n",
    "params['bq_dataset_id'] = \"mars\"\n",
    "params['bq_table_id'] = \"tmp-table-v15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb60c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/202835066335/locations/us-central1/pipelineJobs/bq-fs-export-20220208142302\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/202835066335/locations/us-central1/pipelineJobs/bq-fs-export-20220208142302')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bq-fs-export-20220208142302?project=202835066335\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/202835066335/locations/us-central1/pipelineJobs/bq-fs-export-20220208142302 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/202835066335/locations/us-central1/pipelineJobs/bq-fs-export-20220208142302 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/202835066335/locations/us-central1/pipelineJobs/bq-fs-export-20220208142302 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/202835066335/locations/us-central1/pipelineJobs/bq-fs-export-20220208142302 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform.pipeline_jobs import PipelineJob\n",
    "\n",
    "\n",
    "pl = PipelineJob(\n",
    "        display_name= 'bq-fs-export',\n",
    "        template_path= \"pl.json\",\n",
    "        location='us-central1',\n",
    "        parameter_values=params)\n",
    "\n",
    "pl.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdbc2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import logging\n",
    "\n",
    "bq_location = 'US'\n",
    "bq_project_id = \"feature-store-mars21\"\n",
    "bq_dataset_id = \"mars\"\n",
    "bq_table_id = \"tmp-table-v21\"\n",
    "\n",
    "bq_export_table_id = \"training-v1\"\n",
    "    \n",
    "\n",
    "BUCKET_NAME = \"gs://feature-store-mars21\"\n",
    "\n",
    "bq_query_data_table=\"{project}.{dataset}.{table}\".format(\n",
    "    project=bq_project_id, \n",
    "    dataset=bq_dataset_id, \n",
    "    table=bq_table_id)\n",
    "\n",
    "client = bigquery.Client(project=bq_project_id, location=bq_location,)\n",
    "\n",
    "overwrite_table = False\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE if overwrite_table else bigquery.job.WriteDisposition.WRITE_EMPTY,\n",
    "    destination = bq_query_data_table)\n",
    "\n",
    "try:\n",
    "    query_job = client.query(query = query, \n",
    "                             job_config = job_config)\n",
    "    query_job.result()\n",
    "    #if .total_rows == 0:\n",
    "    #    raise Exception(\"Query return no rows\".format(bq_query_data_table))\n",
    "\n",
    "    if query_job.errors: \n",
    "        raise Exception() \n",
    "except Exception as e:\n",
    "    logging.error(query_job.errors)\n",
    "    raise e\n",
    "\n",
    "table_dataset_metadata={}\n",
    "\n",
    "table = client.get_table(bq_query_data_table)  # Make an API request.\n",
    "table_dataset_path = \"bq://{}\".format(bq_query_data_table)\n",
    "table_dataset_metadata['table_name'] = bq_query_data_table\n",
    "\n",
    "\n",
    "\n",
    "if table.num_rows==0:\n",
    "    raise Exception(\"BQ table {} has no rows. Ensure thet your query returns results: {}\".format(bq_query_data_table, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5985dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict # in case dict is not created using python>=3.6\n",
    "schema = OrderedDict((i.name,i.field_type) for i in table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02fd7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type_cols = []\n",
    "pass_through_cols = []\n",
    "reading_entity_types=True\n",
    "for key, value in schema.items():\n",
    "    if key=='timestamp':\n",
    "        reading_entity_types=False\n",
    "        if value!=\"TIMESTAMP\":\n",
    "            raise ValueError(\"timestamp column must be of type TIMESTAMP\")\n",
    "    else:\n",
    "        if reading_entity_types==True:\n",
    "            entity_type_cols.append(key)\n",
    "        else:\n",
    "            pass_through_cols.append(key)\n",
    "        \n",
    "if reading_entity_types==True: # means timestamp column was not found so this remained False\n",
    "    raise ValueError(\"timestamp column missing from BQ table. It is required for feature store data retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d2ea3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['planets']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_type_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f90ab4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pass_through_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffae74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate entity types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57d1a57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 The Featurestore does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"The Featurestore does not exist.\"\n\tdebug_error_string = \"{\"created\":\"@1644330891.189056255\",\"description\":\"Error received from peer ipv4:142.250.200.10:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1066,\"grpc_message\":\"The Featurestore does not exist.\",\"grpc_status\":5}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3102/1171670555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     client_options={\"api_endpoint\": API_ENDPOINT})\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfs_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madmin_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_entity_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfs_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs_entities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform_v1beta1/services/featurestore_service/client.py\u001b[0m in \u001b[0;36mlist_entity_types\u001b[0;34m(self, request, parent, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;31m# This method is paged; wrap the response in a pager, which provides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 The Featurestore does not exist."
     ]
    }
   ],
   "source": [
    "fs_location = 'us-central1'\n",
    "fs_project = 'feature-store-mars21'\n",
    "fs_featurestore_name = 'universe'\n",
    "\n",
    "fs_path= 'projects/{fs_project}/locations/{fs_location}/featurestores/{fs_name}'.format(fs_project=fs_project,\n",
    "                                                   fs_location=fs_location,\n",
    "                                                   fs_name=fs_featurestore_name)\n",
    "    \n",
    "from google.cloud.aiplatform_v1beta1 import FeaturestoreServiceClient\n",
    "\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(fs_location)\n",
    "\n",
    "admin_client = FeaturestoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "\n",
    "fs_entities = admin_client.list_entity_types(parent=fs_path).entity_types\n",
    "\n",
    "fs_entities = [i.name.split('/')[-1] for i in fs_entities]\n",
    "\n",
    "if len(set(entity_type_cols).difference(fs_entities))>0:\n",
    "    raise ValueError(\"Table column(s) {} before timestamp column do not match entities in feature store {} \".format(entity_type_cols, fs_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "3deb321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from BQ and export to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "90485ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to retrieve for each entity type\n",
    "my_features  = {'planets': [\"avg_max_temp_5d\", \"arr_max_temp_3d\", \"min_temp_std\"]}\n",
    "feature_diff = set(my_features.keys()).difference(entity_type_cols)\n",
    "if len(feature_diff)>0:\n",
    "    raise LookupError(\"Features requested for entities {} that does not exist in filtering query columns: {} \".format(feature_diff, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "1eaeb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bq_export_data_table=\"{project}.{dataset}.{table}\".format(\n",
    "    project=bq_project_id, \n",
    "    dataset=bq_dataset_id, \n",
    "    table=bq_export_table_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "entity_type_cols\n",
    "\n",
    "entity_type_specs_arr=[]\n",
    "\n",
    "# Select features to read\n",
    "for ent_type, features_arr in my_features.items():\n",
    "    entity_type_specs_arr.append(\n",
    "        featurestore_service_pb2.BatchReadFeatureValuesRequest.EntityTypeSpec(\n",
    "            # read feature values of features subscriber_type and duration_minutes from \"bikes\"\n",
    "            entity_type_id= ent_type, \n",
    "            feature_selector= feature_selector_pb2.FeatureSelector(\n",
    "                id_matcher=feature_selector_pb2.IdMatcher(\n",
    "                ids=features_arr))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "batch_serving_request = featurestore_service_pb2.BatchReadFeatureValuesRequest(\n",
    "    featurestore=fs_path,\n",
    "    bigquery_read_instances=BigQuerySource(input_uri = \"bq://{}\".format(bq_query_data_table)),\n",
    "    #csv_read_instances=io_pb2.CsvSource(\n",
    "    #    gcs_source=io_pb2.GcsSource(uris=[FEATURE_REQ_CSV_PATH])),\n",
    "    \n",
    "    # Output info\n",
    "    destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "        bigquery_destination=io_pb2.BigQueryDestination(\n",
    "            # output to BigQuery table\n",
    "            output_uri='bq://{}'.format(bq_export_data_table))),\n",
    "    #destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "    #    tfrecord_destination=io_pb2.CsvDestination(\n",
    "    #        gcs_destination=EXPORT_TF_PATH)),\n",
    "    \n",
    "   entity_type_specs=entity_type_specs_arr\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "9683bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 Destination Table `bq://feature-store-mars21.mars.training-v1` must not exist.\n",
      "CPU times: user 0 ns, sys: 4.2 ms, total: 4.2 ms\n",
      "Wall time: 999 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    print(admin_client.batch_read_feature_values(batch_serving_request).result())\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae66f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
